{
 "cells": [
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-22T14:34:25.176556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import paramiko\n",
    "import io\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# pip install paramiko\n",
    "# pip install python-dotenv\n",
    "# hostname = \"\"\n",
    "# port = 22\n",
    "# username = \"\"\n",
    "# password = \"\"\n",
    "\n",
    "hostname = os.getenv(\"SFTP_HOST\")\n",
    "port = int(os.getenv(\"SFTP_PORT\"))\n",
    "username = os.getenv(\"SFTP_USER\")\n",
    "password = os.getenv(\"SFTP_PASSWORD\")\n",
    "\n",
    "if not password:\n",
    "    raise ValueError(\"‚ùå SFTP_PASSWORD nicht in .env definiert!\")\n",
    "\n",
    "print(f\"üîÑ Verbinde zu {hostname}:{port} als {username}...\")\n",
    "\n",
    "try:\n",
    "    transport = paramiko.Transport((hostname, port))\n",
    "    transport.start_client()\n",
    "    transport.auth_password(username=username, password=password)\n",
    "\n",
    "    if not transport.is_authenticated():\n",
    "        raise paramiko.AuthenticationException(\"Auth failed\")\n",
    "\n",
    "    print(\"‚úÖ SFTP-Verbindung erfolgreich!\")\n",
    "\n",
    "    sftp = paramiko.SFTPClient.from_transport(transport)\n",
    "    sftp.chdir(\"out\")\n",
    "\n",
    "    # === LV.ZIP HERUNTERLADEN ===\n",
    "    print(\"üì• Lade lv.zip...\")\n",
    "    lv_zip_data = io.BytesIO()\n",
    "    sftp.getfo(\"lv.zip\", lv_zip_data)\n",
    "    lv_zip_data.seek(0)\n",
    "    print(f\"  ‚úì {len(lv_zip_data.getvalue()) / (1024**2):.1f} MB\")\n",
    "\n",
    "    # === AKI.ZIP HERUNTERLADEN ===\n",
    "    print(\"üì• Lade aki.zip...\")\n",
    "    aki_zip_data = io.BytesIO()\n",
    "    sftp.getfo(\"aki.zip\", aki_zip_data)\n",
    "    aki_zip_data.seek(0)\n",
    "    print(f\"  ‚úì {len(aki_zip_data.getvalue()) / (1024**2):.1f} MB\")\n",
    "\n",
    "    sftp.close()\n",
    "    transport.close()\n",
    "    print(\"‚úÖ Download abgeschlossen!\")\n",
    "\n",
    "except paramiko.AuthenticationException as e:\n",
    "    raise ValueError(f\"‚ùå Authentifizierung fehlgeschlagen: {e}\")\n",
    "except Exception as e:\n",
    "    raise ConnectionError(f\"‚ùå SFTP-Fehler: {e}\")"
   ],
   "id": "243b9c189178beca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Verbinde zu ftp.bruder-gruppe.de:22 als hochschuleog...\n",
      "‚úÖ SFTP-Verbindung erfolgreich!\n",
      "üì• Lade lv.zip...\n",
      "  ‚úì 440.9 MB\n",
      "üì• Lade aki.zip...\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import zipfile\n",
    "import subprocess\n",
    "\n",
    "os.makedirs(\"./data\", exist_ok=True)\n",
    "\n",
    "# === LV.ZIP ENTPACKEN ===\n",
    "print(\"üì¶ Entpacke lv.zip...\")\n",
    "try:\n",
    "    with zipfile.ZipFile(lv_zip_data, 'r', allowZip64=True) as z:\n",
    "        z.extractall(\"./data\")\n",
    "    print(\"  ‚úì Erfolgreich entpackt\")\n",
    "except NotImplementedError:\n",
    "    print(\"  ‚ö†Ô∏è ZIP-Kompression nicht unterst√ºtzt, nutze Kommandozeile...\")\n",
    "    lv_zip_data.seek(0)\n",
    "    with open(\"/tmp/lv.zip\", \"wb\") as f:\n",
    "        f.write(lv_zip_data.read())\n",
    "    subprocess.run([\"unzip\", \"-o\", \"/tmp/lv.zip\", \"-d\", \"./data\"], check=True)\n",
    "    print(\"  ‚úì Mit unzip entpackt\")\n",
    "\n",
    "# === AKI.ZIP ENTPACKEN ===\n",
    "print(\"üì¶ Entpacke aki.zip...\")\n",
    "try:\n",
    "    with zipfile.ZipFile(aki_zip_data, 'r', allowZip64=True) as z:\n",
    "        z.extractall(\"./data\")\n",
    "    print(\"  ‚úì Erfolgreich entpackt\")\n",
    "except NotImplementedError:\n",
    "    print(\"  ‚ö†Ô∏è ZIP-Kompression nicht unterst√ºtzt, nutze Kommandozeile...\")\n",
    "    aki_zip_data.seek(0)\n",
    "    with open(\"/tmp/aki.zip\", \"wb\") as f:\n",
    "        f.write(aki_zip_data.read())\n",
    "    subprocess.run([\"unzip\", \"-o\", \"/tmp/aki.zip\", \"-d\", \"./data\"], check=True)\n",
    "    print(\"  ‚úì Mit unzip entpackt\")\n",
    "\n",
    "print(\"‚úÖ Alle Dateien entpackt!\")"
   ],
   "id": "70a49b8e49eea85d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === DATAFRAMES LADEN ===\n",
    "dataframes = {}\n",
    "\n",
    "print(\"\\nüìä Lade DataFrames...\")\n",
    "\n",
    "# LV DataFrame\n",
    "lv_path = \"data/lv/lv.csv\"\n",
    "if os.path.exists(lv_path):\n",
    "    dataframes['lv'] = pd.read_csv(lv_path, low_memory=False)\n",
    "    print(f\"  ‚úì lv.csv: {len(dataframes['lv'])} Zeilen\")\n",
    "else:\n",
    "    print(f\"  ‚ùå {lv_path} nicht gefunden!\")\n",
    "\n",
    "# ART DataFrame\n",
    "art_path = \"data/aki/art.csv\"\n",
    "if os.path.exists(art_path):\n",
    "    dataframes['art'] = pd.read_csv(art_path, low_memory=False)\n",
    "    print(f\"  ‚úì art.csv: {len(dataframes['art'])} Zeilen\")\n",
    "else:\n",
    "    print(f\"  ‚ùå {art_path} nicht gefunden!\")\n",
    "\n",
    "print(f\"\\nüéâ Fertig! {len(dataframes)} Datens√§tze geladen.\")\n",
    "print(f\"   Verf√ºgbare DataFrames: {list(dataframes.keys())}\")"
   ],
   "id": "7c18d680011aa8f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Setup & Normalisierung\n",
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "LV_OUT_PATH = os.getenv(\"LV_OUT_PATH\", \"data/lv_clean.csv\")\n",
    "NROWS = int(os.getenv(\"LV_NROWS\", \"0\")) or None\n",
    "\n",
    "if 'lv' in dataframes:\n",
    "    df = dataframes['lv'].copy()\n",
    "else:\n",
    "    raise ValueError(\"'lv' nicht in geladenen Daten gefunden!\")\n",
    "\n",
    "required_cols = {\"id\", \"oz_bez\"}\n",
    "missing = required_cols - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Fehlende Spalten: {missing}\")\n",
    "\n",
    "TRIGGER_PATTERNS = [\n",
    "    r\"\\bwie\\s+(?:oben|vor|zuvor|zuletzt)\\b\",\n",
    "    r\"\\bw(?:ie)?\\s*o\\b\",\n",
    "    r\"\\bdito\\b\",\n",
    "    r\"\\bentspr\\.\\s*oben\\b\",\n",
    "    r\"\\bgleich\\s+wie\\b\",\n",
    "    r\"\\bwie\\s+beschr(?:ieben|.)\\b\",\n",
    "    r\"\\bwie\\s+pos(?:ition)?\\s*\\d+\\b\",\n",
    "    r\"\\bwie\\s+z(?:ur|u)\\s+pos\\b\",\n",
    "    r\"\\bdesgleichen\\b\",\n",
    "]\n",
    "trigger_re = re.compile(\"|\".join(f\"(?:{p})\" for p in TRIGGER_PATTERNS), flags=re.IGNORECASE)\n",
    "\n",
    "def has_trigger(text: str) -> bool:\n",
    "    return bool(trigger_re.search(str(text)))\n",
    "\n",
    "# Neue Spalten zum Inspizieren\n",
    "df[\"Referenz\"] = \"\"\n",
    "df[\"Original\"] = \"\"\n",
    "\n",
    "print(\"‚úì Normalisierung abgeschlossen\")\n",
    "print(f\"  Arbeite mit {len(df)} Zeilen aus 'lv'\")"
   ],
   "id": "8828ae27d2506053",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Referenzen aufl√∂sen & Speichern\n",
    "\n",
    "first_by_id = df.sort_values(\"id\").groupby(\"id\", as_index=True)[\"oz_bez\"].first()\n",
    "\n",
    "def resolve_reference(current_id: int):\n",
    "    search_id = current_id - 1\n",
    "    while search_id in first_by_id.index:\n",
    "        t = first_by_id.loc[search_id]\n",
    "        if not has_trigger(t):\n",
    "            return t\n",
    "        search_id -= 1\n",
    "    return None\n",
    "\n",
    "def resolve_row(row):\n",
    "    text = row[\"oz_bez\"]\n",
    "    row[\"Original\"] = text\n",
    "\n",
    "    if not has_trigger(text):\n",
    "        row[\"Referenz\"] = \"\"\n",
    "        return row\n",
    "\n",
    "    try:\n",
    "        cid = int(row[\"id\"])\n",
    "    except Exception:\n",
    "        row[\"Referenz\"] = \"\"\n",
    "        return row\n",
    "\n",
    "    base = resolve_reference(cid)\n",
    "    if base:\n",
    "        row[\"Referenz\"] = base\n",
    "        # Bereinige Base und Text SEPARAT vor dem Kombinieren\n",
    "        base_clean = re.sub(r\"\\s{2,}\", \" \", base.strip())\n",
    "        text_clean = re.sub(r\"\\s{2,}\", \" \", text.strip())\n",
    "        # Kombiniere mit doppeltem Zeilenumbruch\n",
    "        text = f\"Base: {base_clean}\\n\\nText: {text_clean}\"\n",
    "    else:\n",
    "        # Wenn kein Base gefunden, nur Text bereinigen\n",
    "        text = re.sub(r\"\\s{2,}\", \" \", text.strip())\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "df = df.apply(resolve_row, axis=1)\n",
    "\n",
    "if \"arnr\" in df.columns:\n",
    "    df[\"arnr\"] = df[\"arnr\"].astype(str).str.strip()\n",
    "\n",
    "os.makedirs(os.path.dirname(LV_OUT_PATH), exist_ok=True)\n",
    "df.to_csv(LV_OUT_PATH, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"‚úì Fertig: {LV_OUT_PATH}\")\n",
    "print(f\"  Spalten: Referenz | Original | oz_bez (kombiniert)\")\n",
    "\n"
   ],
   "id": "7df1a91064e192a8",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# %% md\n",
    "# ## Bereinigung - Entferne leere/ung√ºltige oz_bez\n",
    "\n",
    "# %%\n",
    "# √úberpr√ºfe Datenqualit√§t VORHER\n",
    "print(\"üìä Datenqualit√§t VOR Bereinigung:\")\n",
    "print(f\"  Gesamt-Zeilen: {len(df)}\")\n",
    "\n",
    "# Check f√ºr Null-Werte\n",
    "null_count = df['oz_bez'].isna().sum()\n",
    "print(f\"  NULL-Werte: {null_count}\")\n",
    "\n",
    "# Check f√ºr leere Strings\n",
    "empty_strings = (df['oz_bez'] == '').sum()\n",
    "print(f\"  Leere Strings: {empty_strings}\")\n",
    "\n",
    "# Check f√ºr nur Whitespace/Newlines\n",
    "def is_empty_or_whitespace(text):\n",
    "    \"\"\"Pr√ºft ob Text leer, nur Leerzeichen oder nur Newlines ist\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return True\n",
    "    text_str = str(text).strip()\n",
    "    return len(text_str) == 0\n",
    "\n",
    "whitespace_only = df['oz_bez'].apply(is_empty_or_whitespace).sum()\n",
    "print(f\"  Nur Whitespace/Newlines: {whitespace_only}\")\n",
    "\n",
    "# %%"
   ],
   "id": "553f840b9fca0da7",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Entferne alle ung√ºltigen Eintr√§ge\n",
    "print(\"\\nüßπ Entferne ung√ºltige Eintr√§ge...\")\n",
    "\n",
    "# Filter: Behalte nur Zeilen mit g√ºltigem oz_bez\n",
    "df_clean = df[~df['oz_bez'].apply(is_empty_or_whitespace)].copy()\n",
    "\n",
    "removed_count = len(df) - len(df_clean)\n",
    "print(f\"  ‚úì Entfernt: {removed_count} Zeilen\")\n",
    "print(f\"  ‚úì Verbleibend: {len(df_clean)} Zeilen ({len(df_clean)/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Aktualisiere df f√ºr weitere Verarbeitung\n",
    "df = df_clean\n",
    "\n",
    "# %%"
   ],
   "id": "dfb16c7ca1b955d0",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# √úberpr√ºfe Datenqualit√§t NACHHER\n",
    "print(\"\\n‚úÖ Datenqualit√§t NACH Bereinigung:\")\n",
    "print(f\"  Gesamt-Zeilen: {len(df)}\")\n",
    "print(f\"  NULL-Werte in oz_bez: {df['oz_bez'].isna().sum()}\")\n",
    "print(f\"  Leere Strings in oz_bez: {(df['oz_bez'] == '').sum()}\")\n",
    "print(f\"  Nur Whitespace in oz_bez: {df['oz_bez'].apply(is_empty_or_whitespace).sum()}\")\n",
    "\n",
    "if df['oz_bez'].apply(is_empty_or_whitespace).sum() == 0:\n",
    "    print(f\"  üéâ Keine ung√ºltigen Texte mehr!\")"
   ],
   "id": "382044ff5ac986b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %% md\n",
    "# ## 60/20/20 Train/Val/Test Split - TextID gruppiert\n",
    "\n",
    "# %%\n",
    "# Split mit TextID Gruppenbildung\n",
    "print(\"\\nüîÄ F√ºhre 60/20/20 Split durch (TextID-Gruppen bleiben zusammen)...\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Pr√ºfe ob textid Spalte existiert\n",
    "if 'textid' not in df.columns:\n",
    "    print(\"‚ö†Ô∏è  'textid' Spalte nicht gefunden, nutze einfachen Split\")\n",
    "    train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42)\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "else:\n",
    "    # Gruppiere nach textid - nur unique TextIDs splitten\n",
    "    print(\"  ‚ÑπÔ∏è  Gruppiere nach 'textid'...\")\n",
    "    unique_textids = df['textid'].dropna().unique()\n",
    "    print(f\"  ‚ÑπÔ∏è  Unique TextIDs: {len(unique_textids)}\")\n",
    "\n",
    "    # Split die TextID-Gruppen (nicht die einzelnen Rows)\n",
    "    train_textids, temp_textids = train_test_split(\n",
    "        unique_textids,\n",
    "        test_size=0.4,\n",
    "        random_state=42\n",
    "    )\n",
    "    val_textids, test_textids = train_test_split(\n",
    "        temp_textids,\n",
    "        test_size=0.5,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Weise Rows basierend auf ihrer TextID den Sets zu\n",
    "    train_df = df[df['textid'].isin(train_textids)].copy()\n",
    "    val_df = df[df['textid'].isin(val_textids)].copy()\n",
    "    test_df = df[df['textid'].isin(test_textids)].copy()\n",
    "\n",
    "print(f\"\\n  ‚úì Train-Set:      {len(train_df)} Zeilen ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"  ‚úì Validation-Set: {len(val_df)} Zeilen ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"  ‚úì Test-Set:       {len(test_df)} Zeilen ({len(test_df)/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Kontrolle: Keine Overlap zwischen Sets\n",
    "train_textids_set = set(train_df['textid'].dropna().unique())\n",
    "val_textids_set = set(val_df['textid'].dropna().unique())\n",
    "test_textids_set = set(test_df['textid'].dropna().unique())\n",
    "\n",
    "overlap_train_val = len(train_textids_set & val_textids_set)\n",
    "overlap_train_test = len(train_textids_set & test_textids_set)\n",
    "overlap_val_test = len(val_textids_set & test_textids_set)\n",
    "\n",
    "print(f\"\\n  üîç Kontrolle (Overlaps sollten 0 sein):\")\n",
    "print(f\"     Train ‚à© Val:  {overlap_train_val}\")\n",
    "print(f\"     Train ‚à© Test: {overlap_train_test}\")\n",
    "print(f\"     Val ‚à© Test:   {overlap_val_test}\")\n",
    "\n",
    "if overlap_train_val == 0 and overlap_train_test == 0 and overlap_val_test == 0:\n",
    "    print(f\"  ‚úÖ Keine TextID-Overlaps - Split ist sauber!\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è  WARNUNG: Es gibt Overlaps!\")\n",
    "\n",
    "# %%"
   ],
   "id": "cd6b5845e63e1dc0",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Speichern der Splits\n",
    "print(\"\\nüíæ Speichere Splits...\")\n",
    "\n",
    "TRAIN_PATH = os.getenv(\"TRAIN_PATH\", \"data/train.csv\")\n",
    "VAL_PATH = os.getenv(\"VAL_PATH\", \"data/val.csv\")\n",
    "TEST_PATH = os.getenv(\"TEST_PATH\", \"data/test.csv\")\n",
    "\n",
    "# Erstelle Verzeichnis falls n√∂tig\n",
    "os.makedirs(os.path.dirname(TRAIN_PATH) or \".\", exist_ok=True)\n",
    "\n",
    "train_df.to_csv(TRAIN_PATH, index=False, encoding=\"utf-8\")\n",
    "val_df.to_csv(VAL_PATH, index=False, encoding=\"utf-8\")\n",
    "test_df.to_csv(TEST_PATH, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"  ‚úì {TRAIN_PATH}\")\n",
    "print(f\"  ‚úì {VAL_PATH}\")\n",
    "print(f\"  ‚úì {TEST_PATH}\")\n",
    "\n",
    "print(\"\\n‚úÖ Splits erfolgreich erstellt und gespeichert!\")\n"
   ],
   "id": "c2622ad7d89ef658"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
