{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "source": [
    "# %%\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Pfade aus .env\n",
    "TRAIN_PATH = os.getenv(\"TRAIN_PATH\", \"data/train.csv\")\n",
    "VAL_PATH = os.getenv(\"VAL_PATH\", \"data/val.csv\")\n",
    "TEST_PATH = os.getenv(\"TEST_PATH\", \"data/test.csv\")\n",
    "RETRIEVER_OUTPUT_DIR = os.getenv(\"RETRIEVER_OUTPUT_DIR\", \"data/retriever\")\n",
    "\n",
    "Path(RETRIEVER_OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"âœ“ Imports und Konfiguration geladen\")\n",
    "print(f\"  Train: {TRAIN_PATH}\")\n",
    "print(f\"  Val:   {VAL_PATH}\")\n",
    "print(f\"  Test:  {TEST_PATH}\")\n",
    "\n",
    "# %%"
   ],
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Lade die Splits\n",
    "print(\"\\nðŸ“– Lade Daten-Splits...\")\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "val_df = pd.read_csv(VAL_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "\n",
    "print(f\"  âœ“ Train-Set:      {len(train_df)} Dokumente\")\n",
    "print(f\"  âœ“ Validation-Set: {len(val_df)} Dokumente\")\n",
    "print(f\"  âœ“ Test-Set:       {len(test_df)} Dokumente\")\n",
    "\n",
    "total_docs = len(train_df) + len(val_df) + len(test_df)\n",
    "print(f\"\\n  Gesamt: {total_docs} Dokumente\")\n",
    "print(f\"  Spalten: {list(train_df.columns)}\")\n",
    "\n",
    "# %%"
   ],
   "id": "e733331fb9a956a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# TF-IDF Vectorizer trainieren (nur auf Training-Set)\n",
    "print(\"\\nðŸ”§ Trainiere TF-IDF Vectorizer...\")\n",
    "\n",
    "# Kombiniere relevante Text-Spalten\n",
    "train_texts = train_df['oz_bez']\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    min_df=2,\n",
    "    max_df=0.8,\n",
    "    ngram_range=(1, 2),\n",
    "    analyzer='char',\n",
    "    strip_accents='unicode',\n",
    "    lowercase=True\n",
    ")\n",
    "\n",
    "# Fit auf Training-Set\n",
    "tfidf_matrix_train = tfidf.fit_transform(train_texts)\n",
    "print(f\"  âœ“ TF-IDF Vocabulary-GrÃ¶ÃŸe: {len(tfidf.get_feature_names_out())}\")\n",
    "print(f\"  âœ“ Training-Matrix Shape: {tfidf_matrix_train.shape}\")\n",
    "\n",
    "# Speichere Vectorizer\n",
    "with open(f\"{RETRIEVER_OUTPUT_DIR}/tfidf_vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tfidf, f)\n",
    "\n",
    "print(\"  âœ“ Vectorizer gespeichert\")\n",
    "\n",
    "# %%"
   ],
   "id": "71a7ef6a0b06f9b1",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Transformer fÃ¼r Validation & Test\n",
    "print(\"\\nðŸ“Š Transformiere Validation & Test Sets...\")\n",
    "\n",
    "tfidf_matrix_val = tfidf.transform(val_df['oz_bez'])\n",
    "tfidf_matrix_test = tfidf.transform(test_df['oz_bez'])\n",
    "\n",
    "print(f\"  âœ“ Validation-Matrix Shape: {tfidf_matrix_val.shape}\")\n",
    "print(f\"  âœ“ Test-Matrix Shape: {tfidf_matrix_test.shape}\")\n",
    "\n",
    "# Speichern fÃ¼r spÃ¤teren Gebrauch\n",
    "import scipy.sparse as sp\n",
    "sp.save_npz(f\"{RETRIEVER_OUTPUT_DIR}/tfidf_train.npz\", tfidf_matrix_train)\n",
    "sp.save_npz(f\"{RETRIEVER_OUTPUT_DIR}/tfidf_val.npz\", tfidf_matrix_val)\n",
    "sp.save_npz(f\"{RETRIEVER_OUTPUT_DIR}/tfidf_test.npz\", tfidf_matrix_test)\n",
    "\n",
    "# %%"
   ],
   "id": "f55c3e08d195d814",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Retriever Funktion: Top-K Ã¤hnliche Dokumente finden\n",
    "def retrieve_documents(query_text, k=5, tfidf_vect=None, doc_matrix=None, doc_ids=None):\n",
    "    \"\"\"\n",
    "    Findet die Top-K Ã¤hnlichsten Dokumente zu einer Query\n",
    "\n",
    "    Args:\n",
    "        query_text: Query String\n",
    "        k: Anzahl der Top-K Ergebnisse\n",
    "        tfidf_vect: Trainierter TF-IDF Vectorizer\n",
    "        doc_matrix: TF-IDF Matrix der Dokumente\n",
    "        doc_ids: Indices der Dokumente\n",
    "\n",
    "    Returns:\n",
    "        List of (doc_id, score, text) tuples\n",
    "    \"\"\"\n",
    "    query_vec = tfidf_vect.transform([query_text])\n",
    "    similarities = cosine_similarity(query_vec, doc_matrix).flatten()\n",
    "\n",
    "    top_k_indices = np.argsort(similarities)[-k:][::-1]\n",
    "\n",
    "    results = []\n",
    "    for idx in top_k_indices:\n",
    "        results.append({\n",
    "            'index': idx,\n",
    "            'score': similarities[idx],\n",
    "            'doc_id': doc_ids.iloc[idx] if hasattr(doc_ids, 'iloc') else doc_ids[idx]\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# %%"
   ],
   "id": "a597c3fd6b88cc01",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Test: Retriever auf Validation-Set\n",
    "print(\"\\nðŸ§ª Teste Retriever auf Validation-Set...\")\n",
    "\n",
    "# Beispiel-Queries\n",
    "test_queries = [\n",
    "    val_df.iloc[10]['oz_bez'][:50] if len(val_df) > 10 else \"Test\",\n",
    "    val_df.iloc[50]['oz_bez'][:50] if len(val_df) > 50 else \"Test\",\n",
    "    \"schraube bolzen\"\n",
    "]\n",
    "\n",
    "for i, query in enumerate(test_queries):\n",
    "    print(f\"\\n  Query {i+1}: '{query}'\")\n",
    "    results = retrieve_documents(\n",
    "        query,\n",
    "        k=3,\n",
    "        tfidf_vect=tfidf,\n",
    "        doc_matrix=tfidf_matrix_val,\n",
    "        doc_ids=val_df.reset_index(drop=True).index\n",
    "    )\n",
    "\n",
    "    for j, result in enumerate(results):\n",
    "        print(f\"    {j+1}. Score: {result['score']:.4f} | Doc-ID: {result['doc_id']}\")\n",
    "\n",
    "print(\"\\nâœ“ Retriever-Test abgeschlossen\")\n",
    "\n",
    "# %%"
   ],
   "id": "5b64e2a65ff2920d",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Reranker Funktion (Optional: KÃ¶nnte spÃ¤ter mit LLM erweitert werden)\n",
    "def rerank_documents(query_text, retrieved_docs, val_df, method='cosine'):\n",
    "    \"\"\"\n",
    "    Rerankt die bereits retrievten Dokumente\n",
    "\n",
    "    Args:\n",
    "        query_text: Original Query\n",
    "        retrieved_docs: Liste von retrivierten Dokumenten\n",
    "        val_df: DataFrame mit Dokumente\n",
    "        method: Reranking-Methode ('cosine', 'bm25', etc.)\n",
    "\n",
    "    Returns:\n",
    "        Reranked Documents\n",
    "    \"\"\"\n",
    "    # Hier kÃ¶nnten komplexere Reranking-Methoden implementiert werden\n",
    "    # FÃ¼r jetzt: Sortiere nach Score (bereits gemacht im Retriever)\n",
    "\n",
    "    return sorted(retrieved_docs, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "# %%"
   ],
   "id": "7bbc25c31782cb97",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Evaluierungs-Metriken\n",
    "print(\"\\nðŸ“ˆ Evaluierungs-Metriken...\")\n",
    "\n",
    "def evaluate_retriever(queries, ground_truth_indices, retriever_func, k=5):\n",
    "    \"\"\"\n",
    "    Evaluiert den Retriever mit MRR und NDCG\n",
    "    \"\"\"\n",
    "    mrr_scores = []\n",
    "\n",
    "    for query, gt_idx in zip(queries, ground_truth_indices):\n",
    "        results = retriever_func(query, k=k)\n",
    "        retrieved_indices = [r['index'] for r in results]\n",
    "\n",
    "        # Mean Reciprocal Rank\n",
    "        if gt_idx in retrieved_indices:\n",
    "            rank = retrieved_indices.index(gt_idx) + 1\n",
    "            mrr_scores.append(1 / rank)\n",
    "        else:\n",
    "            mrr_scores.append(0)\n",
    "\n",
    "    mean_mrr = np.mean(mrr_scores)\n",
    "    return {'MRR': mean_mrr, 'scores': mrr_scores}\n",
    "\n",
    "print(\"âœ“ Evaluierungs-Funktionen vorbereitet\")\n",
    "\n",
    "# %%"
   ],
   "id": "139b01330fa8fa67",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\nðŸŽ‰ Retriever & Reranker Setup abgeschlossen!\")\n",
    "print(f\"\\n   Speicherort: {RETRIEVER_OUTPUT_DIR}\")\n",
    "print(f\"   - train.csv, val.csv, test.csv\")\n",
    "print(f\"   - tfidf_vectorizer.pkl\")\n",
    "print(f\"   - tfidf_train.npz, tfidf_val.npz, tfidf_test.npz\")"
   ],
   "id": "46469fe388d3f215"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
